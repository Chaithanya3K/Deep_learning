{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPpXY9PMaiiAvLtmLrl7O1p",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Chaithanya3K/Deep_learning/blob/main/Activation_function.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##The sigmoid activation function is used in the output layer for binary classification, where the input is categorized into one of two categories. The output of this function ranges from 0 to 1, not strictly 0 or 1. Once we get the output from the output layer, we manually threshold it to 0 or 1, but the raw output lies between 0 and 1.\n",
        "\n"
      ],
      "metadata": {
        "id": "K0rj4kaaH3Q-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "def sigmoid(x):\n",
        "  return 1/(1+math.exp(-x))"
      ],
      "metadata": {
        "id": "cjlJevjjI8JI"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sigmoid(100)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5tDes5guHT2L",
        "outputId": "7c07e216-fb05-481b-a1d6-4a906e465555"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1.0"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sigmoid(-4)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r2g1TXZ1Ho9W",
        "outputId": "dad71b62-01fd-40ca-b5d5-89895a030e0d"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.01798620996209156"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##The tanh activation function is commonly used in the hidden layers of a neural network. Its output ranges from -1 to 1, making it zero-centered(consider negative values), which helps the model learn faster compared to the sigmoid function. Positive inputs are mapped to values between 0 and 1, while negative inputs are mapped to values between -1 and 0."
      ],
      "metadata": {
        "id": "S52ncnxNJBzL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def tanh(x):\n",
        "  return ((math.exp(x)-math.exp(-x))/(math.exp(x)+math.exp(-x)))"
      ],
      "metadata": {
        "id": "es7sNBqgHtaz"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tanh(-11)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2SJOI7E5Htds",
        "outputId": "65a63bd9-4098-4165-ca5a-03279fc34e7c"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "-0.9999999994421065"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tanh(50)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xI9TeVxqKlfV",
        "outputId": "dce17395-8948-428d-fcb3-b16c7580343f"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1.0"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##The ReLU (Rectified Linear Unit) activation function is widely used in the hidden layers of neural networks because it reduces computational cost and helps the model train faster. It outputs 0 for all negative input values, and for positive inputs, it returns the same value."
      ],
      "metadata": {
        "id": "GS62JCqqKtPT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def relu(x):\n",
        "  return max(0,x)"
      ],
      "metadata": {
        "id": "SAaN6mrGKlhS"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "relu(-4)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RRaHhhwqLbiN",
        "outputId": "9ab8d040-248b-4b3c-ba08-bbf2a3f96f37"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "relu(9.8)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jP5hxLYcLblD",
        "outputId": "7d61adcb-34f8-4889-e1f7-ae241aa7f861"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "9.8"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def leaky_relu(x):\n",
        "  return max(0.1*x,x);"
      ],
      "metadata": {
        "id": "k75D7jROLgVg"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "leaky_relu(-100)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ufc4TrddLgYV",
        "outputId": "c277d9ae-5381-4805-ff55-db96730977c9"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "-10.0"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    }
  ]
}