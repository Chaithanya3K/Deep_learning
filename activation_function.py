# -*- coding: utf-8 -*-
"""Activation_function.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1oXHmjxEmQygfi1Fs8eSKBuT-kyOzjhJ0

##The sigmoid activation function is used in the output layer for binary classification, where the input is categorized into one of two categories. The output of this function ranges from 0 to 1, not strictly 0 or 1. Once we get the output from the output layer, we manually threshold it to 0 or 1, but the raw output lies between 0 and 1.
"""

import math
def sigmoid(x):
  return 1/(1+math.exp(-x))

sigmoid(100)

sigmoid(-4)

"""##The tanh activation function is commonly used in the hidden layers of a neural network. Its output ranges from -1 to 1, making it zero-centered(consider negative values), which helps the model learn faster compared to the sigmoid function. Positive inputs are mapped to values between 0 and 1, while negative inputs are mapped to values between -1 and 0."""

def tanh(x):
  return ((math.exp(x)-math.exp(-x))/(math.exp(x)+math.exp(-x)))

tanh(-11)

tanh(50)

"""##The ReLU (Rectified Linear Unit) activation function is widely used in the hidden layers of neural networks because it reduces computational cost and helps the model train faster. It outputs 0 for all negative input values, and for positive inputs, it returns the same value."""

def relu(x):
  return max(0,x)

relu(-4)

relu(9.8)

def leaky_relu(x):
  return max(0.1*x,x);

leaky_relu(-100)